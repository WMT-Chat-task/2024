<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Challenge Sets Subtask &middot; WMT Metrics Shared Task
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          WMT Metrics Shared Task
        </a>
      </h1>
      <p class="lead">This shared task will examine automatic evaluation metrics for machine translation.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Metrics Task</a>
      

      
      
        
          
        
      
        
      
        
          
            <a class="sidebar-nav-item active" href="/subtasks/challenge/">Challenge Sets Subtask</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/subtasks/mbr/">MBR Subtask</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/subtasks/qe/">QE Subtask</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/subtasks/resources/">Previous Editions & Resources</a>
          
        
      
        
      
        
      
      
      <a class="sidebar-nav-item" href="https://github.com/WMT-Metrics-task/">GitHub project</a>
    </nav>

    <p>&copy; 2022. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Challenge Sets Subtask</h1>
  <ul>
  <li><a href="/">Home</a></li>
</ul>

<p>Every year the Metrics shared task has been pushing for better automatic MT metrics and in the last few years we have seen great progress with metrics achieving much higher correlations with human judgements <a href="https://aclanthology.org/2020.wmt-1.77/">(Mathur et al., 2020;</a> <a href="https://aclanthology.org/2021.wmt-1.73/">Freitag et al., 2021)</a>. Yet, while the limitations of metrics such as BLEU are well known in the MT comunity, we still do not know which limitations new metrics (specially neural ones) might have. For this reason we created this subtask!</p>

<p>Inspired by the <a href="https://bibinlp.umiacs.umd.edu/sharedtask.html">Build it, Break it: The Language Edition</a> we created a subtask where participants (<em>breakers</em>) are asked to build challenging examples that target specific phenomena currently not addresses by current. On top of that we also encourage paper submissions on Metrics analysis (for inspiration: <a href="https://aclanthology.org/2021.wmt-1.59/">A Fine-Grained Analysis of BERTScore</a>).</p>

<p>This shared task will have the two following rounds:</p>

<p><strong>1) Breaking Round:</strong>  Participants (Breakers) will be given access to source sentences and two different human references that they can play around to create challenging examples for metrics. They must send the resulting “challenge sets” to the organizers.</p>

<p><strong>2) Scoring Round:</strong> The challenge sets created by Breakers will be sent to all Metrics participants/Builders to score. Also, the organizers will score all the data with baseline metrics such as BLEU, chrF, BERTScore, COMET, BLEURT, Prism and YiSi-1.</p>

<p><strong>3) Analysis Round:</strong> Breakers will receive their data with all the metrics scores for analysis.</p>

<h2 id="challenge-set-structure">Challenge Set Structure:</h2>

<p>We expect submissions in a tab-separated values (TSV) with the following format:</p>

<h3 id="format">Format:</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">source</th>
      <th style="text-align: center">good-translation</th>
      <th style="text-align: center">incorrect-translation</th>
      <th style="text-align: center">reference</th>
      <th style="text-align: center">phenomena</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Das Shampoo hilft gegen Schuppen.</td>
      <td style="text-align: center">The shampoo helps against dandruff.</td>
      <td style="text-align: center">The shampoo helps against flakes.</td>
      <td style="text-align: center">The shampoo helps fight dandruff.</td>
      <td style="text-align: center">lexical-ambiguity</td>
    </tr>
  </tbody>
</table>

<p>where the <code class="language-plaintext highlighter-rouge">source</code> column contains the original segment, <code class="language-plaintext highlighter-rouge">good-translation</code> column contains a correct translation for the phenomena (in the above case is a lexical ambiguity), <code class="language-plaintext highlighter-rouge">incorrect-translation</code> an example of a translation that is not correct according to a specific phenomena and a <code class="language-plaintext highlighter-rouge">reference</code> column with (ideally) human translations, <code class="language-plaintext highlighter-rouge">phenomena</code> is an identifier that identifies the phenomena being tested.</p>

<p>For all challenge sets we will report kendall tau-like similar to what we have done in <a href="https://aclanthology.org/2021.wmt-1.73/">WMT 2021</a>. This formula measures how many times a given metric scores the <code class="language-plaintext highlighter-rouge">good-translation</code> above the <code class="language-plaintext highlighter-rouge">incorrect-translation</code> and it is defined as follows:</p>

<p align="center">
    <img src="/public/css/kendall-tau.png" alt="Kendall Tau-like Formula" style="height: 75px;" />
</p>

<h2 id="important-dates">Important Dates:</h2>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">Date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Breaking Round</em> data release</td>
      <td style="text-align: center">TBA</td>
    </tr>
    <tr>
      <td><em>Breaking Round</em> submission deadline</td>
      <td style="text-align: center">TBA</td>
    </tr>
    <tr>
      <td><em>Scoring Round</em></td>
      <td style="text-align: center">TBA</td>
    </tr>
    <tr>
      <td><em>Analysis Round</em></td>
      <td style="text-align: center">TBA</td>
    </tr>
    <tr>
      <td>Paper submission deadline to WMT</td>
      <td style="text-align: center">TBA</td>
    </tr>
    <tr>
      <td>WMT Notification of acceptance</td>
      <td style="text-align: center">TBA</td>
    </tr>
    <tr>
      <td>WMT Camera-ready deadline</td>
      <td style="text-align: center">TBA</td>
    </tr>
    <tr>
      <td>Conference</td>
      <td style="text-align: center">TBA</td>
    </tr>
  </tbody>
</table>


</div>

    </div>

  </body>
</html>
